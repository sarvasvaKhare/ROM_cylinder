{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarvasvaKhare/ROM_cylinder/blob/main/ROM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1nwL_cGrCPdVO15G10vfr7DznMHOZiEIw' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1nwL_cGrCPdVO15G10vfr7DznMHOZiEIw\" -O ParaView-v5.9.1.tar.gz && rm -rf /tmp/cookies.txt\n",
        "!sudo mv ParaView-v5.9.1.tar.gz /opt/\n",
        "%cd /opt\n",
        "!sudo tar xf ParaView-v5.9.1.tar.gz\n",
        "!sudo rm ParaView-v5.9.1.tar.gz\n",
        "%cd ParaView-5.9.1-MPI-Linux-Python3.8-64bit\n",
        "import sys\n",
        "sys.path.insert(0, \"/opt/ParaView-5.9.1-MPI-Linux-Python3.8-64bit/lib/python3.8/site-packages\")\n",
        "!wget https://cloud.tu-braunschweig.de/s/sJYEfzFG7yDg3QT/download/datasets_13_09_2022.tar.gz\n",
        "!tar xzf datasets_13_09_2022.tar.gz\n",
        "!sudo rm datasets_13_09_2022.tar.gz"
      ],
      "metadata": {
        "id": "MkBo4Yyyaqya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b1491f0-51da-4091-8035-dde4000438cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-08 19:59:03--  https://docs.google.com/uc?export=download&confirm=t&id=1nwL_cGrCPdVO15G10vfr7DznMHOZiEIw\n",
            "Resolving docs.google.com (docs.google.com)... 173.194.215.113, 173.194.215.139, 173.194.215.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|173.194.215.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/obs5pci1rg3k2rmcco7nef5d0sj3sj41/1678305525000/11687854275746841412/*/1nwL_cGrCPdVO15G10vfr7DznMHOZiEIw?e=download&uuid=b2ef01da-70c1-4f48-baec-77834a3adf19 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-08 19:59:03--  https://doc-0s-5g-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/obs5pci1rg3k2rmcco7nef5d0sj3sj41/1678305525000/11687854275746841412/*/1nwL_cGrCPdVO15G10vfr7DznMHOZiEIw?e=download&uuid=b2ef01da-70c1-4f48-baec-77834a3adf19\n",
            "Resolving doc-0s-5g-docs.googleusercontent.com (doc-0s-5g-docs.googleusercontent.com)... 173.194.210.132, 2607:f8b0:400c:c0f::84\n",
            "Connecting to doc-0s-5g-docs.googleusercontent.com (doc-0s-5g-docs.googleusercontent.com)|173.194.210.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 518515478 (494M) [application/x-gzip]\n",
            "Saving to: ‘ParaView-v5.9.1.tar.gz’\n",
            "\n",
            "ParaView-v5.9.1.tar 100%[===================>] 494.49M  78.3MB/s    in 5.8s    \n",
            "\n",
            "2023-03-08 19:59:09 (85.2 MB/s) - ‘ParaView-v5.9.1.tar.gz’ saved [518515478/518515478]\n",
            "\n",
            "/opt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl7GMjHTCUzY"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, Dict\n",
        "from itertools import groupby\n",
        "from collections import defaultdict\n",
        "import torch as pt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "!pip3 install git+https://github.com/FlowModelingControl/flowtorch\n",
        "import flowtorch\n",
        "from flowtorch import DATASETS\n",
        "from flowtorch.data import FOAMDataloader, mask_box\n",
        "\n",
        "# increase plot resolution\n",
        "plt.rcParams[\"figure.dpi\"] = 160\n",
        "\n",
        "# make results reproducible\n",
        "pt.manual_seed(0)\n",
        "\n",
        "# create output directory\n",
        "output = \"output\"\n",
        "!mkdir -p $output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0-ipyMVMYMk"
      },
      "outputs": [],
      "source": [
        "def create_normal_cluster(center: Tuple[float], stdev: Tuple[float],\n",
        "                          n_points: int) -> pt.Tensor:\n",
        "    \"\"\"Create cluster of normally distributed points.\n",
        "    \"\"\"\n",
        "    assert len(center) == len(stdev)\n",
        "    n_dim = len(center)\n",
        "    cluster = pt.zeros((n_points, n_dim))\n",
        "    for i, (mean, std) in enumerate(zip(center, stdev)):\n",
        "        cluster[:, i] = pt.normal(mean, std, size=(n_points,))\n",
        "    return cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaPl2f6usnnn"
      },
      "outputs": [],
      "source": [
        "cluster_1 = create_normal_cluster((1.0, 1.0), (1.0, 1.0), 100)\n",
        "cluster_2 = create_normal_cluster((4.0, 1.0), (0.5, 1.0), 50)\n",
        "cluster_3 = create_normal_cluster((1.0, 5.0), (1.0, 0.5), 25)\n",
        "plt.scatter(cluster_1[:, 0], cluster_1[:, 1], marker=\"o\", label=\"cluster 1\")\n",
        "plt.scatter(cluster_2[:, 0], cluster_2[:, 1], marker=\"x\", label=\"cluster 2\")\n",
        "plt.scatter(cluster_3[:, 0], cluster_3[:, 1], marker=\"+\", label=\"cluster 3\")\n",
        "plt.scatter((1.0, 4.0, 1.0), (1.0, 1.0, 5.0), marker=\"*\", c=\"C3\", s=100, label=\"centroids\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.legend()\n",
        "plt.savefig(f\"{output}/clustering_test_data.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fljfz8M6s2I_"
      },
      "outputs": [],
      "source": [
        "data = pt.cat((cluster_1, cluster_2, cluster_3))\n",
        "rows_shuffled = pt.randperm(data.shape[0])\n",
        "data = data[rows_shuffled]\n",
        "\n",
        "plt.scatter(data[:, 0], data[:, 1], marker=\"o\", label=\"raw data\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.legend()\n",
        "plt.savefig(f\"{output}/clustering_test_data_no_labels.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNDhn4nZtBSh"
      },
      "outputs": [],
      "source": [
        "def initialize_centroids_randomly(k: int, data: pt.Tensor) -> pt.Tensor:\n",
        "    \"\"\"Randomly select data points as initial centroids.\n",
        "    \"\"\"\n",
        "    n_points = data.shape[0]\n",
        "    probs = pt.ones(n_points) / n_points\n",
        "    rows = pt.multinomial(probs, k)\n",
        "    return data[rows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jA9c_2DStHto"
      },
      "outputs": [],
      "source": [
        "centroids = initialize_centroids_randomly(3, data)\n",
        "\n",
        "plt.scatter(data[:, 0], data[:, 1], marker=\"o\", label=\"raw data\")\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], marker=\"*\", c=\"C3\", s=100, label=\"initial centroids\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.legend()\n",
        "plt.savefig(f\"{output}/random_initial_centroids.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb2aadrTtXvH"
      },
      "outputs": [],
      "source": [
        "def find_nearest_centroid(centroids: pt.Tensor, data: pt.Tensor) -> pt.Tensor:\n",
        "    \"\"\"Find the id of the nearest centroid for each data point.\n",
        "    \"\"\"\n",
        "    n_points = data.shape[0]\n",
        "    n_centroids = centroids.shape[0]\n",
        "    labels = pt.zeros(n_points, dtype=pt.int64)\n",
        "    distance = pt.zeros((n_points, n_centroids))\n",
        "    for i in range(n_centroids):\n",
        "        distance[:, i] = pt.linalg.norm(data - centroids[i], dim=1)\n",
        "    return pt.argmin(distance, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-nEdXRytMy8"
      },
      "outputs": [],
      "source": [
        "initial_labels = find_nearest_centroid(centroids, data)\n",
        "cl_1 = data[initial_labels == 0]\n",
        "cl_2 = data[initial_labels == 1]\n",
        "cl_3 = data[initial_labels == 2]\n",
        "plt.scatter(cl_1[:, 0], cl_1[:, 1], marker=\"o\", label=\"initial cluster 1\")\n",
        "plt.scatter(cl_2[:, 0], cl_2[:, 1], marker=\"x\", label=\"initial cluster 2\")\n",
        "plt.scatter(cl_3[:, 0], cl_3[:, 1], marker=\"+\", label=\"initial cluster 3\")\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], marker=\"*\", c=\"C3\", s=100, label=\"initial centroids\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.legend()\n",
        "plt.savefig(f\"{output}/intial_cluster_labels.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zi_tXJNxuPUp"
      },
      "outputs": [],
      "source": [
        "def update_centroids(centroids: pt.Tensor, data: pt.Tensor) -> pt.Tensor:\n",
        "    \"\"\"Update centroid position based on cluster mean value.\n",
        "    \"\"\"\n",
        "    n_centroids = centroids.shape[0]\n",
        "    new_centroids = pt.zeros_like(centroids)\n",
        "    cluster_ids = find_nearest_centroid(centroids, data)\n",
        "    for i in range(n_centroids):\n",
        "        new_centroids[i] = data[cluster_ids == i].mean(dim=0)\n",
        "    return new_centroids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG7_HTEnuRMt"
      },
      "outputs": [],
      "source": [
        "new_centroids = update_centroids(centroids, data)\n",
        "new_labels = find_nearest_centroid(new_centroids, data)\n",
        "cl_1 = data[new_labels == 0]\n",
        "cl_2 = data[new_labels == 1]\n",
        "cl_3 = data[new_labels == 2]\n",
        "plt.scatter(cl_1[:, 0], cl_1[:, 1], marker=\"o\", label=\"new cluster 1\")\n",
        "plt.scatter(cl_2[:, 0], cl_2[:, 1], marker=\"x\", label=\"new cluster 2\")\n",
        "plt.scatter(cl_3[:, 0], cl_3[:, 1], marker=\"+\", label=\"new cluster 3\")\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], marker=\"*\", edgecolors=\"C3\", s=100, facecolors=\"none\", label=\"initial centroids\")\n",
        "plt.scatter(new_centroids[:, 0], new_centroids[:, 1], marker=\"*\", c=\"C3\", s=100, label=\"updated centroids\")\n",
        "for i in range(centroids.shape[0]):\n",
        "    plt.annotate(\"\", xy=new_centroids[i], xytext=centroids[i], arrowprops=dict(arrowstyle=\"->\"))\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.legend()\n",
        "plt.savefig(f\"{output}/updated_cluster_labels.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAC4DoxQudA_"
      },
      "outputs": [],
      "source": [
        "max_iter = 10\n",
        "tol = 1.0e-4\n",
        "\n",
        "centroids_hist = []\n",
        "centroids_hist.append(initialize_centroids_randomly(3, data))\n",
        "for it in range(max_iter):\n",
        "    centroids_hist.append(update_centroids(centroids_hist[-1], data))\n",
        "    mean_diff = pt.linalg.norm(centroids_hist[-1]-centroids_hist[-2], dim=1).mean()\n",
        "    if mean_diff < tol:\n",
        "        print(f\"Clustering converged after {it+1} iterations.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkjOxhsmuqXG"
      },
      "outputs": [],
      "source": [
        "fig, axarr = plt.subplots(1, 8, figsize=(8, 3), sharex=True, sharey=True)\n",
        "for i, ct in enumerate(centroids_hist[:7]):\n",
        "    labels = find_nearest_centroid(centroids_hist[i], data)\n",
        "    cl_1 = data[labels == 0]\n",
        "    cl_2 = data[labels == 1]\n",
        "    cl_3 = data[labels == 2]\n",
        "    axarr[i].scatter(cl_1[:, 0], cl_1[:, 1], marker=\"o\")\n",
        "    axarr[i].scatter(cl_2[:, 0], cl_2[:, 1], marker=\"x\")\n",
        "    axarr[i].scatter(cl_3[:, 0], cl_3[:, 1], marker=\"+\")\n",
        "    axarr[i].scatter(ct[:, 0], ct[:, 1], marker=\"*\", c=\"C3\", s=100)\n",
        "    if i > 0:\n",
        "        axarr[i].scatter(centroids_hist[i-1][:, 0], centroids_hist[i-1][:, 1], marker=\"*\", edgecolors=\"C3\", facecolors=\"none\", s=100)\n",
        "        for j in range(centroids_hist[-1].shape[0]):\n",
        "            axarr[i].annotate(\"\", xy=centroids_hist[i][j], xytext=centroids_hist[i-1][j], arrowprops=dict(arrowstyle=\"->\"))\n",
        "    axarr[i].set_xlabel(r\"$x_1$\")\n",
        "    axarr[i].set_title(f\"iteration {i}\")\n",
        "axarr[0].set_ylabel(r\"$x_2$\")\n",
        "\n",
        "plt.savefig(f\"{output}/kmeans_iterations.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWQl6Ag3wajM"
      },
      "outputs": [],
      "source": [
        "bad_centroids = pt.tensor([[2.5, 1.0], [1.0, 5.0], [2.0, 5.0]])\n",
        "initial_labels = find_nearest_centroid(bad_centroids, data)\n",
        "cl_1 = data[initial_labels == 0]\n",
        "cl_2 = data[initial_labels == 1]\n",
        "cl_3 = data[initial_labels == 2]\n",
        "plt.scatter(cl_1[:, 0], cl_1[:, 1], marker=\"o\", label=\"initial cluster 1\")\n",
        "plt.scatter(cl_2[:, 0], cl_2[:, 1], marker=\"x\", label=\"initial cluster 2\")\n",
        "plt.scatter(cl_3[:, 0], cl_3[:, 1], marker=\"+\", label=\"initial cluster 3\")\n",
        "plt.scatter(bad_centroids[:, 0], bad_centroids[:, 1], marker=\"*\", c=\"C3\", s=100, label=\"initial centroids\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.legend()\n",
        "plt.savefig(f\"{output}/bad_intial_centroids.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWCgCUuGwgtz"
      },
      "outputs": [],
      "source": [
        "max_iter = 10\n",
        "tol = 1.0e-4\n",
        "\n",
        "bad_centroids_hist = []\n",
        "bad_centroids_hist.append(bad_centroids)\n",
        "for it in range(max_iter):\n",
        "    bad_centroids_hist.append(update_centroids(bad_centroids_hist[-1], data))\n",
        "    mean_diff = pt.linalg.norm(bad_centroids_hist[-1]-bad_centroids_hist[-2], dim=1).mean()\n",
        "    if mean_diff < tol:\n",
        "        print(f\"Clustering converged after {it+1} iterations.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpda0J9ewlAk"
      },
      "outputs": [],
      "source": [
        "final_labels = find_nearest_centroid(bad_centroids_hist[-1], data)\n",
        "cl_1 = data[final_labels == 0]\n",
        "cl_2 = data[final_labels == 1]\n",
        "cl_3 = data[final_labels == 2]\n",
        "plt.scatter(cl_1[:, 0], cl_1[:, 1], marker=\"o\", label=\"final cluster 1\")\n",
        "plt.scatter(cl_2[:, 0], cl_2[:, 1], marker=\"x\", label=\"final cluster 2\")\n",
        "plt.scatter(cl_3[:, 0], cl_3[:, 1], marker=\"+\", label=\"final cluster 3\")\n",
        "plt.scatter(bad_centroids_hist[-1][:, 0], bad_centroids_hist[-1][:, 1], marker=\"*\", c=\"C3\", s=100, label=\"final centroids\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.legend()\n",
        "plt.savefig(f\"{output}/bad_intial_centroids_final.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MigFQkdvw6Vc"
      },
      "outputs": [],
      "source": [
        "def compute_cluster_inertia(centroids: pt.Tensor, data: pt.Tensor) -> float:\n",
        "    \"\"\"Compute sum of squared distances over all clusters.\n",
        "    \"\"\"\n",
        "    labels = find_nearest_centroid(centroids, data)\n",
        "    inertia = 0.0\n",
        "    for i in range(data.shape[1]):\n",
        "        inertia += pt.linalg.norm(data[labels==i]-centroids[i], dim=1).square().sum()\n",
        "    return inertia\n",
        "print(\"Inertia for random initialization: {:2.4f}\".format(compute_cluster_inertia(centroids_hist[-1], data)))\n",
        "print(\"Inertia for bad initialization: {:2.4f}\".format(compute_cluster_inertia(bad_centroids_hist[-1], data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcyMJ6irxELk"
      },
      "outputs": [],
      "source": [
        "def initialize_centroids_improved(k: int, data: pt.Tensor) -> pt.Tensor:\n",
        "    \"\"\"Randomly select data points as initial centroids.\n",
        "    \"\"\"\n",
        "    n_points = data.shape[0]\n",
        "    probs = pt.ones(n_points) / n_points\n",
        "    rows = pt.zeros(k, dtype=pt.int64)\n",
        "    rows[0] = pt.multinomial(probs, 1)\n",
        "    distance = pt.zeros((n_points, k-1))\n",
        "    for i in range(1, k):\n",
        "        distance[:, i-1] = pt.linalg.norm(data-data[rows[i-1]], dim=1).square()\n",
        "        min_dist = distance[:, :i].min(dim=1).values\n",
        "        probs = min_dist / min_dist.sum()\n",
        "        rows[i] = pt.multinomial(probs, 1)\n",
        "    return data[rows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJSwnI74yjJq"
      },
      "outputs": [],
      "source": [
        "centroids = initialize_centroids_improved(3, data)\n",
        "\n",
        "plt.scatter(data[:, 0], data[:, 1], marker=\"o\", label=\"raw data\")\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], marker=\"*\", c=\"C3\", s=100, label=\"initial centroids\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.legend()\n",
        "plt.savefig(f\"{output}/improved_initial_centroids.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWgRcx_3yrGX"
      },
      "outputs": [],
      "source": [
        "initialization_methods = {\n",
        "    \"random\" : initialize_centroids_randomly,\n",
        "    \"kmeans++\" : initialize_centroids_improved\n",
        "}\n",
        "\n",
        "def find_centroids(k: int, data: pt.Tensor, max_iter: int=100,\n",
        "                   tol: float=1.0e-6, init: str=\"random\", verbose=False) -> Tuple[pt.Tensor, int]:\n",
        "    centroids = initialization_methods[init](k, data)\n",
        "    for i in range(max_iter):\n",
        "        old_centroids = centroids[:]\n",
        "        centroids = update_centroids(centroids, data)\n",
        "        mean_diff = pt.linalg.norm(centroids-old_centroids, dim=1).mean()\n",
        "        if mean_diff < tol:\n",
        "            if verbose:\n",
        "                print(f\"Clustering converged after {i+1} iterations.\")\n",
        "            break\n",
        "    return centroids, i+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7vv4KCEyzsV"
      },
      "outputs": [],
      "source": [
        "repeat = 100\n",
        "inertia_random, inertia_improved = [], []\n",
        "iter_random, iter_improved = [], []\n",
        "\n",
        "for _ in range(repeat):\n",
        "    centroids, it = find_centroids(3, data, init=\"random\")\n",
        "    inertia_random.append(compute_cluster_inertia(centroids, data).item())\n",
        "    iter_random.append(it)\n",
        "    centroids, it = find_centroids(3, data, init=\"kmeans++\")\n",
        "    inertia_improved.append(compute_cluster_inertia(centroids, data).item())\n",
        "    iter_improved.append(it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEkK1rGty_JI"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "ax1.boxplot((inertia_random, inertia_improved), labels=(\"random\", \"k-means++\"))\n",
        "ax2.boxplot((iter_random, iter_improved), labels=(\"random\", \"k-means++\"))\n",
        "ax1.set_ylabel(\"inertia\")\n",
        "ax2.set_ylabel(\"iterations\")\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.savefig(f\"{output}/random_vs_improved.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFSTwQZMzJBe"
      },
      "outputs": [],
      "source": [
        "t = pt.linspace(0.0, 2.0*np.pi, 100)\n",
        "data = pt.zeros((100, 2))\n",
        "data[:, 0] = pt.sin(t) + pt.normal(0.0, 0.05, (100,))\n",
        "data[:, 1] = pt.cos(t) + pt.normal(0.0, 0.05, (100,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBPL1ZbIzTdk"
      },
      "outputs": [],
      "source": [
        "plt.scatter(t, data[:, 0], label=r\"$x_1$\")\n",
        "plt.scatter(t, data[:, 1], label=r\"$x_2$\")\n",
        "plt.xlabel(r\"$t$\")\n",
        "plt.legend()\n",
        "plt.xlim(0.0, 2.0*np.pi)\n",
        "plt.savefig(f\"{output}/periodic_dataset_raw_time.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkuVTVQKzWii"
      },
      "outputs": [],
      "source": [
        "clustering = KMeans(n_clusters=10, random_state=2)\n",
        "clustering.fit(data.numpy())\n",
        "cluster_ids = clustering.predict(data.numpy())\n",
        "centroids = clustering.cluster_centers_\n",
        "\n",
        "plt.scatter(data[:, 0], data[:, 1], c=cluster_ids, marker=\"x\", cmap=\"jet\", label=\"raw data\")\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c=range(10), marker=\"o\", s=100, cmap=\"jet\", label=\"centroids\")\n",
        "for i, c in enumerate(centroids):\n",
        "    plt.text(c[0], c[1]+0.05, f\"{i}\")\n",
        "plt.gca().set_aspect(\"equal\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.legend()\n",
        "plt.savefig(f\"{output}/periodic_dataset_clustering.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uilK8nzqzlGi"
      },
      "outputs": [],
      "source": [
        "def remove_sequential_duplicates(sequence: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Get sequence of integers without sequential duplicates.\n",
        "    \"\"\"\n",
        "    is_different = np.diff(sequence).astype(bool)\n",
        "    return sequence[np.insert(is_different, 0, True)]\n",
        "\n",
        "def compute_transition_time(cluster_sequence: np.ndarray,\n",
        "                            dt: float, verbose=False) -> Dict[str, float]:\n",
        "    \"\"\"Compute transition time between cluster centroids.\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"sequence: \", cluster_sequence)\n",
        "    centroid_sequence = remove_sequential_duplicates(cluster_sequence)\n",
        "    if verbose:\n",
        "        print(\"centroid sequence: \", centroid_sequence)\n",
        "    seq_duplicates = np.array(\n",
        "            [sum(1 for _ in group)\n",
        "             for _, group in groupby(cluster_sequence)]\n",
        "    )\n",
        "    if verbose:\n",
        "        print(\"sequential duplicates: \", seq_duplicates)\n",
        "    transition = defaultdict(list)\n",
        "    for i in range(len(centroid_sequence)-1):\n",
        "        key = \",\".join(map(str, centroid_sequence[i:i+2]))\n",
        "        transition[key].append(\n",
        "            0.5 * dt *\n",
        "            np.sum(seq_duplicates[i:i+2])\n",
        "        )\n",
        "    return {key: np.mean(value) for key, value in transition.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDXf2uJ5Aks5"
      },
      "outputs": [],
      "source": [
        "transition_times = compute_transition_time(cluster_ids, 2.0*np.pi/100, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qe1TqaqbA6eD"
      },
      "outputs": [],
      "source": [
        "transition_times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY9SANkdA_MB"
      },
      "outputs": [],
      "source": [
        "def get_next_cluster(current: int, transitions: list) -> int:\n",
        "    for t in transitions:\n",
        "        if t.startswith(str(current)):\n",
        "            return int(t[-1:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aC_yy0fsBCls"
      },
      "outputs": [],
      "source": [
        "def simulate(centroids: np.ndarray, transition_times: Dict[str, float],\n",
        "             start_id: int, end_time) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    visited_centroids, time = [start_id], [0.0]\n",
        "    while time[-1] < end_time:\n",
        "        visited_centroids.append(get_next_cluster(visited_centroids[-1], transition_times.keys()))\n",
        "        transition = \"{:d},{:d}\".format(*visited_centroids[-2:])\n",
        "        time.append(time[-1] + transition_times[transition])\n",
        "        if time[-1] > end_time:\n",
        "            break\n",
        "    visited = np.zeros((len(time), centroids.shape[1]))\n",
        "    for i, ci in enumerate(visited_centroids):\n",
        "        visited[i] = centroids[ci]\n",
        "    return visited, np.array(time)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqIGeERyBF5V"
      },
      "outputs": [],
      "source": [
        "prediction, times = simulate(centroids, transition_times, clustering.labels_[0], 4*np.pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbO3OogZBNlt"
      },
      "outputs": [],
      "source": [
        "plt.plot(times, prediction[:, 0], label=r\"$\\hat{x}_1$\")\n",
        "plt.plot(times, prediction[:, 1], label=r\"$\\hat{x}_2$\")\n",
        "plt.scatter(t, data[:, 0], label=r\"$x_1$\")\n",
        "plt.scatter(t, data[:, 1], label=r\"$x_2$\")\n",
        "plt.xlabel(r\"$t$\")\n",
        "plt.legend()\n",
        "plt.xlim(0.0, 2*np.pi)\n",
        "plt.savefig(f\"{output}/periodic_dataset_simulated.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrDLdzOYOvaX"
      },
      "outputs": [],
      "source": [
        "clustering = KMeans(n_clusters=20, random_state=0)\n",
        "clustering.fit(data.numpy())\n",
        "cluster_ids = clustering.predict(data.numpy())\n",
        "centroids = clustering.cluster_centers_\n",
        "\n",
        "plt.scatter(data[:, 0], data[:, 1], c=cluster_ids, marker=\"x\", cmap=\"jet\", label=\"raw data\")\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c=range(20), marker=\"o\", s=100, cmap=\"jet\", label=\"centroids\")\n",
        "for i, c in enumerate(centroids):\n",
        "    plt.text(c[0], c[1]+0.05, f\"{i}\")\n",
        "plt.gca().set_aspect(\"equal\")\n",
        "plt.xlabel(r\"$x_1$\")\n",
        "plt.ylabel(r\"$x_2$\")\n",
        "plt.legend()\n",
        "plt.savefig(f\"{output}/periodic_dataset_clustering_refined.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDo01WmgO4Y9"
      },
      "outputs": [],
      "source": [
        "transition_times = compute_transition_time(cluster_ids, 2.0*np.pi/100)\n",
        "transition_times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpHB1aVcPBRd"
      },
      "outputs": [],
      "source": [
        "def compute_transition_probabilities(centroid_sequence: np.ndarray,\n",
        "                                   verbose: bool=False) -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Compute transition probability between pairs of clusters.\n",
        "    \"\"\"\n",
        "    prob = defaultdict(list)\n",
        "    for i in range(len(centroid_sequence)-1):\n",
        "        prob[str(centroid_sequence[i])].append(centroid_sequence[i+1])\n",
        "    if verbose:\n",
        "        print(\"Possible next clusters: \", prob)\n",
        "    for key, next_clusters in prob.items():\n",
        "        unique, counts = np.unique(next_clusters, return_counts=True)\n",
        "        prob[key] = np.stack((unique, counts/counts.sum())).T\n",
        "    return prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-g5BH177PF3L"
      },
      "outputs": [],
      "source": [
        "centroid_sequence = remove_sequential_duplicates(clustering.labels_)\n",
        "transition_probs = compute_transition_probabilities(centroid_sequence, verbose=True)\n",
        "transition_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xzxDAp7PTKO"
      },
      "outputs": [],
      "source": [
        "def sample_next_cluster(current: int, transition_probs) -> int:\n",
        "    \"\"\"Sample the next cluster probabilisticly.\n",
        "    \"\"\"\n",
        "    key = str(current)\n",
        "    return int(np.random.choice(\n",
        "        transition_probs[key][:, 0], p=transition_probs[key][:, 1])\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9TN5cRDPWv9"
      },
      "outputs": [],
      "source": [
        "for _ in range(10):\n",
        "    print(sample_next_cluster(10, transition_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1If8czVSPe3P"
      },
      "outputs": [],
      "source": [
        "def simulate_probablisticly(centroids: np.ndarray,\n",
        "                            transition_probs: Dict[str, np.ndarray],\n",
        "                            transition_times: Dict[str, float],\n",
        "                            start_id: int,\n",
        "                            end_time: float) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    visited_centroids, time = [start_id], [0.0]\n",
        "    while time[-1] < end_time:\n",
        "        visited_centroids.append(sample_next_cluster(visited_centroids[-1], transition_probs))\n",
        "        transition = \"{:d},{:d}\".format(*visited_centroids[-2:])\n",
        "        time.append(time[-1] + transition_times[transition])\n",
        "        if time[-1] > end_time:\n",
        "            break\n",
        "    visited = np.zeros((len(time), centroids.shape[1]))\n",
        "    for i, ci in enumerate(visited_centroids):\n",
        "        visited[i] = centroids[ci]\n",
        "    return visited, np.array(time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnSaaiNFTcyW"
      },
      "outputs": [],
      "source": [
        "prediction, times = simulate_probablisticly(centroids, transition_probs, transition_times, clustering.labels_[0], 4*np.pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STcIcGm2TfsT"
      },
      "outputs": [],
      "source": [
        "plt.plot(times, prediction[:, 0], label=r\"$\\hat{x}_1$\")\n",
        "plt.plot(times, prediction[:, 1], label=r\"$\\hat{x}_2$\")\n",
        "plt.scatter(t, data[:, 0], label=r\"$x_1$\")\n",
        "plt.scatter(t, data[:, 1], label=r\"$x_2$\")\n",
        "plt.xlabel(r\"$t$\")\n",
        "plt.legend()\n",
        "plt.xlim(0.0, 2*np.pi)\n",
        "plt.savefig(f\"{output}/periodic_dataset_simulated_refined.svg\", bbox_inches=\"tight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfuKGpk20OlT"
      },
      "outputs": [],
      "source": [
        "DATASETS=flowtorch.constants.find_datasets(\"/opt/ParaView-5.9.1-MPI-Linux-Python3.8-64bit/datasets/\")\n",
        "path = DATASETS[\"of_cylinder2D_binary\"]\n",
        "loader = FOAMDataloader(path)\n",
        "times = loader.write_times\n",
        "window_times = [time for time in times if float(time) >= 4.0]\n",
        "\n",
        "# load vertices, discard z-coordinate, and create a mask\n",
        "vertices = loader.vertices[:, :2]\n",
        "mask = mask_box(vertices, lower=[0.1, -1], upper=[0.75, 1])\n",
        "\n",
        "# assemble data matrix\n",
        "data_matrix = pt.zeros((mask.sum().item(), len(window_times)), dtype=pt.float32)\n",
        "for i, time in enumerate(window_times):\n",
        "    # load the vorticity vector field, take the z-component [:, 2], and apply the mask\n",
        "    data_matrix[:, i] = pt.masked_select(loader.load_snapshot(\"vorticity\", time)[:, 2], mask)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(data_matrix: pt.Tensor, rank: int) -> Tuple[pt.Tensor, pt.Tensor]:\n",
        "    U, s, VH = pt.linalg.svd(data_matrix, full_matrices=False)\n",
        "    return U[:, :rank], pt.diag(s[:rank]) @ VH[:rank, :]"
      ],
      "metadata": {
        "id": "WH6r4MXvCd8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modes, coeff = encode(data_matrix, rank=20)\n",
        "print(\"Mode matrix shape: \", modes.shape)\n",
        "print(\"Coeff. matrix shape: \", coeff.shape)"
      ],
      "metadata": {
        "id": "AjoZ3sOLaAYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering = KMeans(n_clusters=40, random_state=0)\n",
        "clustering.fit(coeff.T.numpy())\n",
        "centroids = clustering.cluster_centers_\n",
        "# transition times\n",
        "times_num = np.array([float(t) for t in window_times])\n",
        "dt = times_num[1] - times_num[0]\n",
        "transition_times = compute_transition_time(clustering.labels_, dt)\n",
        "# transition probabilities\n",
        "centroid_sequence = remove_sequential_duplicates(clustering.labels_)\n",
        "transition_probs = compute_transition_probabilities(centroid_sequence)"
      ],
      "metadata": {
        "id": "5lNGD_btaD1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction, times_sim = simulate_probablisticly(centroids, transition_probs, transition_times, clustering.labels_[0], 10)"
      ],
      "metadata": {
        "id": "WniaTW9kaKcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, sharex=True)\n",
        "ax1.plot(times_num-times_num[0], coeff[0, :], label=r\"$a_1$\")\n",
        "ax2.plot(times_num-times_num[0], coeff[1, :], label=r\"$a_2$\")\n",
        "ax1.plot(times_sim, prediction[:, 0], ls=\"--\", label=r\"$\\hat{a}_1$\")\n",
        "ax2.plot(times_sim, prediction[:, 1], ls=\"--\", label=r\"$\\hat{a}_2$\")\n",
        "ax2.set_xlabel(r\"$t$ in $s$\")\n",
        "ax2.set_xlim(0, 10)\n",
        "ax1.legend(ncol=2)\n",
        "ax2.legend(ncol=2)\n",
        "plt.savefig(f\"{output}/cylinder_coeff_prediction_cnm.svg\", bbox_inches=\"tight\")"
      ],
      "metadata": {
        "id": "UuJO-lBYaLTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(modes: pt.Tensor, coeff: pt.Tensor) -> pt.Tensor:\n",
        "    return modes @ coeff"
      ],
      "metadata": {
        "id": "91TKsepsaOnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstruction = decode(modes, pt.from_numpy(prediction).T.type(pt.float32))"
      ],
      "metadata": {
        "id": "OC07BOg2aSl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_mode(ax, mode, title, every=4):\n",
        "    ax.tricontourf(x[::every], y[::every], mode[::every], levels=15, cmap=\"jet\")\n",
        "    ax.tricontour(x[::every], y[::every], mode[::every], levels=15, linewidths=0.1, colors='k')\n",
        "    ax.add_patch(plt.Circle((0.2, 0.2), 0.05, color='k'))\n",
        "    ax.set_aspect(\"equal\", 'box')\n",
        "    ax.set_title(title)\n",
        "\n",
        "x = pt.masked_select(vertices[:, 0], mask)\n",
        "y = pt.masked_select(vertices[:, 1], mask)"
      ],
      "metadata": {
        "id": "eN8XDCrLaVpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axarr = plt.subplots(3, 2, figsize=(6, 4), sharex=True, sharey=True)\n",
        "count = 0\n",
        "for row in range(3):\n",
        "    add_mode(axarr[row, 0], reconstruction[:, count], f\"t={times_sim[count]:2.2f}s\")\n",
        "    count += 5\n",
        "    add_mode(axarr[row, 1], reconstruction[:, count], f\"t={times_sim[count]:2.2f}s\")\n",
        "    count += 5\n",
        "plt.savefig(f\"{output}/cylinder_full_prediction_cnm.svg\", bbox_inches=\"tight\")"
      ],
      "metadata": {
        "id": "BdhMptM8acUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flowtorch.data import CSVDataloader\n",
        "\n",
        "data = \"../datasets/naca0012_buffet/surface/\"\n",
        "loader = CSVDataloader.from_foam_surface(data, \"total(p)_coeff_airfoil.raw\")"
      ],
      "metadata": {
        "id": "bEns_UAPLmko"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}